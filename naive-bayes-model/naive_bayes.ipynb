{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"naive_bayes.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1pEOq7SRr6CvHlKmvKB0HFbwgje0TV2yt","authorship_tag":"ABX9TyPf72W2sY9oJaD3WhoGCW19"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"fZxl9qEJZnw4","colab_type":"text"},"source":["## **Naive Bayes**"]},{"cell_type":"markdown","metadata":{"id":"oi6chtphZqnq","colab_type":"text"},"source":["In this notebook, we will use Naive Bayes for sentiment analysis on tweets. Given a tweet, we will decide if it has a positive sentiment or a negative sentiment. In this notenook, we will:\n","\n","* Train a Naive Byaes model on sentiment analysis task.\n","* Test using our model.\n","* Compute ratio of positive words to negative words.\n","* Do error analysis\n","* At last some playaround with random tweets."]},{"cell_type":"markdown","metadata":{"id":"c4_wMbJ0aapv","colab_type":"text"},"source":["#### **Load required libraries and NLTK Twitter sample dataset**"]},{"cell_type":"code","metadata":{"id":"MInAUAbkaeFR","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593665690020,"user_tz":-480,"elapsed":2934,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["!cp '/content/drive/My Drive/Colab Notebooks/NLP-with-classification-and-vector-spaces/Naive-Bayes/utils.py' '/content'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmqHWM8HZh_F","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593665695807,"user_tz":-480,"elapsed":2360,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["from utils import process_tweet, lookup\n","import pdb\n","from nltk.corpus import stopwords, twitter_samples\n","import numpy as np\n","import pandas as pd\n","import nltk\n","import string\n","from nltk.tokenize import TweetTokenizer\n","from os import getcwd"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"lR1ODToqb4jf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1593665702393,"user_tz":-480,"elapsed":3487,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"ec8e5ad5-466b-4cbf-cfe0-37a55a7c110a"},"source":["nltk.download('stopwords')\n","nltk.download('twitter_samples')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"dNeUVKIMb_Kg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593665804227,"user_tz":-480,"elapsed":1514,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["# load positive and negative tweets\n","pos_tweets = twitter_samples.strings('positive_tweets.json')\n","neg_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","# split into train and test set\n","train_pos = pos_tweets[0:4000]\n","test_pos = pos_tweets[4000:]\n","train_neg = neg_tweets[0:4000]\n","test_neg = neg_tweets[4000:]\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg\n","\n","# create train and test labels\n","train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n","test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"28rIK906ei9v","colab_type":"text"},"source":["### **Create a function *word_label_count()***"]},{"cell_type":"markdown","metadata":{"id":"04zgYcZReuqB","colab_type":"text"},"source":["This function will take a list of tweets as input, cleans all of them, and returns a dictionary in the form {(word, label):count}.\n","\n","* The key in the dictionary is a tuple containing the stemmed word and its class label(0 or 1).\n","* The value is the number of times the word appears in the given collection of tweets (an integer).\n","* We will use the `process_tweet` function that was imported above, and then store the words in their respective dictionaries and sets.\n","* We may find it useful to use the `zip` function to match each element in `tweets` with each element in `ys`."]},{"cell_type":"code","metadata":{"id":"WiSRGwgKc6rS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593665908061,"user_tz":-480,"elapsed":1010,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def word_label_count(result, tweets, label):\n","    '''\n","    Input:\n","        result: a dictionary that will be used to map each pair to its frequency\n","        tweets: a list of tweets\n","        label: a list corresponding to the sentiment of each tweet (either 0 or 1)\n","    Output:\n","        result: a dictionary mapping each pair to its frequency\n","    '''\n","\n","    for y, tweet in zip(label, tweets):\n","        for word in process_tweet(tweet):\n","            # define the key, which is the word and label tuple\n","            pair = (word, y)\n","\n","            # if the key exists in the dictionary, increment the count\n","            if pair in result:\n","                result[pair] += 1\n","\n","            # else, if the key is new, add it to the dictionary and set the count to 1\n","            else:\n","                result[pair] = 1\n","\n","    return result"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfIZFxmnfkhX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593666962071,"user_tz":-480,"elapsed":1107,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"334a070a-41b4-4b92-f8d7-5d8a101a50d0"},"source":["# test the function\n","result = {}\n","tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n","ys = [1, 0, 0, 0, 0]\n","word_label_count(result, tweets, ys)"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{('happi', 1): 1, ('sad', 0): 1, ('tire', 0): 2, ('trick', 0): 1}"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"G18HTIWHf--o","colab_type":"text"},"source":["### **Naive Bayes model function**"]},{"cell_type":"markdown","metadata":{"id":"jcYHWu9_gDuf","colab_type":"text"},"source":["Naive bayes is an algorithm that could be used for sentiment analysis. It takes a short time to train and also has a short prediction time.\n","\n","So how do we train a Naive Bayes classifier?\n","* The first part of training a naive bayes classifier is to identify the number of classes that we have.\n","* We will create a probability for each class. $P(D_{pos})$ is the probability that the document is positive. $P(D_{neg})$ is the probability that the document is negative. We will use the below formulas and store the values in a dictionary:\n","$$P(D_{pos}) = \\frac{D_{pos}}{D}$$\n","\n"," $$P(D_{neg}) = \\frac{D_{neg}}{D}$$\n","where $D$ is the total number of documents, or tweets in this case, $D_{pos}$ is the total number of positive tweets and $D_{neg}$ is the total number of negative tweets."]},{"cell_type":"markdown","metadata":{"id":"OcTq6qVOiizW","colab_type":"text"},"source":["**Prior and Logprior**\n","\n","The prior probability represents the underlying probability in the target population that a tweet is positive versus negative. In other words, if we had no specific information and blindly picked a tweet out of the population set, what is the probability that it will be positive versus that it will be negative? That is the \"prior\".\n","\n","The prior is the ratio of the probabilities $\\frac{P(D_{pos})}{P(D_{neg})}$. We take the log of the prior to rescale it, and we call this the logprior\n","\n","$$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$$\n",".\n","\n","Note that $log(\\frac{A}{B})$ is the same as $log(A) - log(B)$. So the logprior can also be calculated as the difference between two logs:\n","\n","$$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})$$"]},{"cell_type":"markdown","metadata":{"id":"KcUlPFlUiAnd","colab_type":"text"},"source":["**Positive and Negative Probability of a Word**\n","\n","To compute the positive probability and the negative probability for a specific word in the vocabulary, we'll use the following inputs:\n","\n","$freq_{pos}$ and $freq_{neg}$ are the frequencies of that specific word in the positive or negative class. In other words, the positive frequency of a word is the number of times the word is counted with the label of 1.\n","$N_{pos}$ and $N_{neg}$ are the total number of positive and negative words for all documents (for all tweets), respectively.\n","$V$ is the number of unique words in the entire set of documents, for all classes, whether positive or negative.\n","We'll use these to compute the positive and negative probability for a specific word using this formula:\n","\n","$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V} $$$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V} $$\n","Notice that we add the \"+1\" in the numerator for additive smoothing. This wiki article explains more about additive smoothing:\n","\n","https://en.wikipedia.org/wiki/Additive_smoothing"]},{"cell_type":"markdown","metadata":{"id":"li_kpDQriAMo","colab_type":"text"},"source":["**Log likelihood**\n","\n","To compute the loglikelihood of that very same word, we can implement the following equations:\n","\n","$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)$$"]},{"cell_type":"markdown","metadata":{"id":"U3Nb4A6rh_PO","colab_type":"text"},"source":["**Create *freqs* dictionary**\n","\n","* Given your count_tweets() function, you can compute a dictionary called freqs that contains all the frequencies.\n","* In this freqs dictionary, the key is the tuple (word, label)\n","* The value is the number of times it has appeared."]},{"cell_type":"code","metadata":{"id":"2GjEzV2NfsAK","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593666969245,"user_tz":-480,"elapsed":3894,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["freqs = word_label_count({}, train_x, train_y)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SX_hG6s05RyX","colab_type":"text"},"source":["Given a freqs dictionary, `train_x` (a list of tweets) and a `train_y` (a list of labels for each tweet), we will calculate the following to implement a naive bayes classifier.\n","\n","\n","**Calculate $V$**\n","\n","* We can compute the number of unique words that appear in the freqs dictionary to get our $V$.\n","\n","**Calculate $freq_{pos}$ and $freq_{neg}$**\n","\n","* Using our `freqs` dictionary, we can compute the positive and negative frequency of each word $freq_{pos}$ and $freq_{neg}$.\n","\n","**Calculate $N_{pos}$ and $N_{neg}$**\n","\n","* Using `freqs` dictionary, we can also compute the total number of positive words $N_{pos}$ and total number of negative words $N_{neg}$.\n","\n","**Calculate $D$, $D_{pos}$, $D_{neg}$**\n","\n","* Using the `train_y` input list of labels, we can calculate the number of documents (tweets) $D$, as well as the number of positive documents (tweets) $D_{pos}$ and number of negative documents (tweets) $D_{neg}$.\n","\n","* Then we can calculate the probability that a document (tweet) is positive $P(D_{pos})$, and the probability that a document (tweet) is negative $P(D_{neg})$.\n","\n","**Calculate the logprior**\n","\n","* The logprior is $log(D_{pos}) - log(D_{neg})$\n","\n","**Calculate log likelihood**\n","\n","* Finally, we can iterate over each word in the vocabulary, and use our `lookup` function (from utils.py) to get the positive frequencies, $freq_{pos}$, and the negative frequencies, $freq_{neg}$, for that specific word.\n","* After that we can compute the positive probability of each word $P(W_{pos})$, negative probability of each word $P(W_{neg})$ using below equations:\n","\n","$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\n","$$$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V} $$\n","\n","*(Hint: We'll use a dictionary to store the log likelihoods for each word. The key is the word, the value is the log likelihood of that word).*\n","\n","* At last we can then compute the loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$.\n"]},{"cell_type":"code","metadata":{"id":"tXyzF2qbi7vu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593666894659,"user_tz":-480,"elapsed":1230,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def train_naive_bayes(freqs, train_x, train_y):\n","    '''\n","    Returns the logprior value and loglikelihood dictionary.\n","    Input:\n","        freqs: dictionary from (word, label) to how often the word appears\n","        train_x: a list of tweets\n","        train_y: a list of labels correponding to the tweets (0,1)\n","    Output:\n","        logprior: the log prior\n","        loglikelihood: the log likelihood of you Naive bayes equation\n","    '''\n","    loglikelihood = {}\n","    logprior = 0\n","\n","    # calculate V, the number of unique words in the vocabulary\n","    vocab = set([pair[0] for pair in freqs.keys()])\n","    V = len(vocab)\n","\n","    # calculate N_pos, N_neg, V_pos, V_neg\n","    N_pos = N_neg = 0\n","    for pair in freqs.keys():\n","        # if the label is positive (greater than zero)\n","        if pair[1] > 0:\n","            # Increment the number of positive words by the count for this (word, label) pair\n","            N_pos += freqs.get(pair)\n","\n","        # else, the label is negative\n","        else:\n","            # increment the number of negative words by the count for this (word,label) pair\n","            N_neg += freqs.get(pair)\n","\n","    # Calculate D, the number of documents\n","    D = len(train_y)\n","\n","    # Calculate D_pos, the number of positive documents\n","    D_pos = sum(i == 1 for i in train_y)\n","\n","    # Calculate D_neg, the number of negative documents\n","    D_neg = sum(i == 0 for i in train_y)\n","\n","    # Calculate logprior\n","    logprior = np.log(D_pos) - np.log(D_neg)\n","\n","    # For each word in the vocabulary...\n","    for word in vocab:\n","        # get the positive and negative frequency of the word\n","        freq_pos = lookup(freqs, word, 1)\n","        freq_neg = lookup(freqs, word, 0)\n","\n","        # calculate the probability that each word is positive, and negative\n","        p_w_pos = (freq_pos + 1) / (N_pos + V)\n","        p_w_neg = (freq_neg + 1) / (N_neg + V)\n","\n","        # calculate the log likelihood of the word\n","        loglikelihood[word] = np.log(p_w_pos) - np.log(p_w_neg)\n","\n","    return logprior, loglikelihood"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"SAaSwr6V8vYY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593667039297,"user_tz":-480,"elapsed":1214,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"4eeadee3-22e3-4f45-f278-056fb4ce398d"},"source":["# calculate logprior and loglikelyhood\n","logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n","print('logprior =', logprior)\n","print('length of loglikelyhood dictionary =', len(loglikelihood))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["logprior = 0.0\n","length of loglikelyhood dictionary = 9089\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1ZpAw4OI9Ywz","colab_type":"text"},"source":["### **Test our Naive Bayes model**"]},{"cell_type":"markdown","metadata":{"id":"wig4L3Qc9jxP","colab_type":"text"},"source":["Now that we have the logprior and loglikelihood, we can test the naive bayes function by making predicting on some tweets!\n","\n","**Create naive_bayes_predict function to make predictions on tweets.**\n","\n","* The function takes in a tweet, logprior, loglikelihood.\n","* It returns the probability that the tweet belongs to the positive or negative class.\n","* For each tweet, sum up loglikelihoods of each word in the tweet.\n","* Also add the logprior to this sum to get the predicted sentiment of that tweet.\n","$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n","\n","*[We calculate the prior (or logprior) from the training data, and in our case the training data is evenly split between positive and negative labels (4000 positive and 4000 negative tweets). This means that the ratio of positive to negative (called prior) is 1, and hence the logprior is 0.\n","\n","The value of 0.0 means that when we add the logprior to the log likelihood, we're just adding zero to the log likelihood.\n","\n","However, we need to remember to always include the logprior, because whenever the data is not perfectly balanced, the logprior will be a non-zero value."]},{"cell_type":"code","metadata":{"id":"0tF5ctnN88KF","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593669002245,"user_tz":-480,"elapsed":1141,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def predict_naive_bayes(tweet, logprior, loglikelihood):\n","    '''\n","    Returns the probability(sum of logprior and loglikelyhood of all the words in a tweet) of a tweet.\n","    Input:\n","        tweet: a string\n","        logprior: a number\n","        loglikelihood: a dictionary of words mapping to numbers\n","    Output:\n","        p: the sum of all the logliklihoods of each word in the tweet (if found in the dictionary) + logprior (a number)\n","\n","    '''\n","\n","    # process the tweet to get a list of words\n","    word_l = process_tweet(tweet)\n","\n","    # initialize probability to zero\n","    p = 0\n","\n","    # add the logprior\n","    p += logprior\n","\n","    for word in word_l:\n","\n","        # check if the word exists in the loglikelihood dictionary\n","        if word in loglikelihood:\n","            # add the log likelihood of that word to the probability\n","            p += loglikelihood[word]\n","\n","    return p"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUXA3ZQ9-5CB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593667526839,"user_tz":-480,"elapsed":992,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"78fc7c8f-4aae-43e5-adde-d653279cbef3"},"source":["# test the function\n","new_tweet = 'Keep smiling.'\n","p = predict_naive_bayes(new_tweet, logprior, loglikelihood)\n","print('The expected output is', p)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["The expected output is 2.1390555947214125\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FipC_7QEDRhS","colab_type":"text"},"source":["**Create *test_naive_bayes* function**\n","\n","* The function takes in `test_x`, `test_y`, `logprior`, and `loglikelihood`\n","* It returns the accuracy of your model.\n","* It uses `predict_naive_bayes` function to make predictions for each tweet in `text_x`."]},{"cell_type":"code","metadata":{"id":"A8l9W8lh_Jx2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593668899410,"user_tz":-480,"elapsed":1173,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n","    \"\"\"\n","    Returns the accuracy of Naive Bayes classifier on test data.\n","    Input:\n","        test_x: A list of tweets\n","        test_y: the corresponding labels for the list of tweets\n","        logprior: the logprior\n","        loglikelihood: a dictionary with the loglikelihoods for each word\n","    Output:\n","        accuracy: (# of tweets classified correctly)/(total # of tweets)\n","    \"\"\"\n","    accuracy = 0\n","\n","    y_hats = []\n","    for tweet in test_x:\n","        # if the prediction is > 0\n","        if predict_naive_bayes(tweet, logprior, loglikelihood) > 0:\n","            # the predicted class is 1\n","            y_hat_i = 1\n","        else:\n","            # otherwise the predicted class is 0\n","            y_hat_i = 0\n","\n","        # append the predicted class to the list y_hats\n","        y_hats.append(y_hat_i)\n","\n","    # error is the average of the absolute values of the differences between y_hats and test_y\n","    error = np.mean(np.abs(y_hats - test_y))\n","\n","    # Accuracy is 1 minus the error\n","    accuracy = 1 - error\n","\n","    return accuracy"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"id":"1VJrM-N4ELnm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593668902684,"user_tz":-480,"elapsed":1782,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"b0ba5018-c869-4603-f8c7-c30577b83b81"},"source":["# calculate the test accuracy\n","print(\"Naive Bayes accuracy = %0.4f\" %\n","      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Naive Bayes accuracy = 0.9940\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"T5dJlqqYFIst","colab_type":"text"},"source":["### **Filter words by Ratio of positive to negative counts**\n","\n","* Some words have more positive counts than others, and can be considered \"more positive\". Likewise, some words can be considered more negative than others.\n","* One way for us to define the level of positiveness or negativeness, without calculating the log likelihood, is to compare the positive to negative frequency of the word.\n","\n","  (*We can also use the log likelihood calculations to compare relative positivity or negativity of words.*)\n","\n","* We can calculate the ratio of positive to negative frequencies of a word.\n","* Once we're able to calculate these ratios, we can also filter a subset of words that have a minimum ratio of positivity / negativity or higher.\n","* Similarly, we can also filter a subset of words that have a maximum ratio of positivity / negativity or lower (words that are at least as negative, or even more negative than a given threshold).\n","\n","In order to do this, we will define a function *get_ratio()*.\n","\n","\n","**Create *get_ratio()* function**\n","\n","* Given the `freqs` dictionary of words and a particular word,we can use `lookup`(freqs,word,1) to get the positive count of the word.\n","* Similarly, we can use the `lookup()` function to get the negative count of that word.\n","* Then we can calculate the ratio of positive divided by negative counts using\n","\n","$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n","  where pos_words and neg_words correspond to the frequency of the words in their respective classes."]},{"cell_type":"code","metadata":{"id":"9iMj3QcbEQn2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593669539484,"user_tz":-480,"elapsed":964,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def get_ratio(freqs, word):\n","    '''\n","    Returns a dictionary with positive_count, negative_count, and their ratio {pos, neg, ratio}.\n","    Input:\n","        freqs: dictionary containing the words\n","        word: string to lookup\n","\n","    Output: a dictionary with keys 'positive', 'negative', and 'ratio'.\n","        Example: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n","    '''\n","    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n","    \n","    # use lookup() to find positive counts for the word (denoted by the integer 1)\n","    pos_neg_ratio['positive'] = lookup(freqs, word, 1)\n","\n","    # use lookup() to find negative counts for the word (denoted by integer 0)\n","    pos_neg_ratio['negative'] = lookup(freqs, word, 0)\n","\n","    # calculate the ratio of positive to negative counts for the word\n","    pos_neg_ratio['ratio'] = (pos_neg_ratio['positive'] + 1) / (pos_neg_ratio['negative'] + 1)\n","\n","    return pos_neg_ratio"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"7bMK5G3XG1Ji","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593669557754,"user_tz":-480,"elapsed":1219,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"ce6db57d-2b9a-4799-8944-d2970b1cbed9"},"source":["# test the function\n","get_ratio(freqs, 'happi')"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'negative': 18, 'positive': 161, 'ratio': 8.526315789473685}"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"7ON03OJNH5ha","colab_type":"text"},"source":["**Create *get_words_by_threshold(freqs,label,threshold)* function**\n","\n","* If we set the label to 1, then we'll look for all words whose threshold of positive/negative is at least as high as that threshold, or higher.\n","* If we set the label to 0, then we'll look for all words whose threshold of positive/negative is at most as low as the given threshold, or lower.\n","* We can use the `get_ratio()` function to get a dictionary containing the positive count, negative count, and the ratio of positive to negative counts.\n","* We can then add that dictionary to aother dictionary, where the key is the word, and the value is the dictionary `pos_neg_ratio` that is returned by the `get_ratio()` function. \n","\n","  An example key-value pair would have this structure:\n","\n","  {'happi':\n","\n","  {'positive': 10, 'negative': 20, 'ratio': 0.5}\n","\n","  }"]},{"cell_type":"code","metadata":{"id":"KQakG_dvG5gP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593670167986,"user_tz":-480,"elapsed":1050,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def get_words_by_threshold(freqs, label, threshold):\n","    '''\n","    Returns a dictionary of dictionary, where key is a word, and value is a dictionary returned by get_ratio() function.\n","    Input:\n","        freqs: dictionary of words\n","        label: 1 for positive, 0 for negative\n","        threshold: ratio that will be used as the cutoff for including a word in the returned dictionary\n","    Output:\n","        word_set: dictionary containing the word and information on its positive count, negative count, \n","        and ratio of positive to negative counts.\n","        example of a key value pair:\n","        {'happi':\n","            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n","        }\n","    '''\n","    word_list = {}\n","\n","    for key in freqs.keys():\n","        word, _ = key\n","\n","        # get the positive/negative ratio for a word\n","        pos_neg_ratio = get_ratio(freqs, word)\n","\n","        # if the label is 1 and the ratio is greater than or equal to the threshold...\n","        if label == 1 and pos_neg_ratio['ratio'] >= threshold:\n","\n","            # Add the pos_neg_ratio to the dictionary\n","            word_list[word] = pos_neg_ratio\n","\n","        # If the label is 0 and the pos_neg_ratio is less than or equal to the threshold...\n","        elif label == 0 and pos_neg_ratio['ratio'] <= threshold:\n","\n","            # Add the pos_neg_ratio to the dictionary\n","            word_list[word] = pos_neg_ratio\n","            \n","    return word_list"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"sYn7ipG_JOlI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1593670183535,"user_tz":-480,"elapsed":1246,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"0fb0f01b-f32d-4438-d970-093e5143b79b"},"source":["# Test the function: find negative words at or below a threshold (set label to 0)\n","get_words_by_threshold(freqs, label=0, threshold=0.05)"],"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'26': {'negative': 20, 'positive': 0, 'ratio': 0.047619047619047616},\n"," ':(': {'negative': 3663, 'positive': 1, 'ratio': 0.0005458515283842794},\n"," ':-(': {'negative': 378, 'positive': 0, 'ratio': 0.002638522427440633},\n"," '>:(': {'negative': 43, 'positive': 0, 'ratio': 0.022727272727272728},\n"," 'beli̇ev': {'negative': 35, 'positive': 0, 'ratio': 0.027777777777777776},\n"," 'justi̇n': {'negative': 35, 'positive': 0, 'ratio': 0.027777777777777776},\n"," 'lost': {'negative': 19, 'positive': 0, 'ratio': 0.05},\n"," 'wi̇ll': {'negative': 35, 'positive': 0, 'ratio': 0.027777777777777776},\n"," 'zayniscomingbackonjuli': {'negative': 19, 'positive': 0, 'ratio': 0.05},\n"," '♛': {'negative': 210, 'positive': 0, 'ratio': 0.004739336492890996},\n"," '》': {'negative': 210, 'positive': 0, 'ratio': 0.004739336492890996},\n"," 'ｍｅ': {'negative': 35, 'positive': 0, 'ratio': 0.027777777777777776},\n"," 'ｓｅｅ': {'negative': 35, 'positive': 0, 'ratio': 0.027777777777777776}}"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"dsGwE394JSUp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":408},"executionInfo":{"status":"ok","timestamp":1593670212114,"user_tz":-480,"elapsed":1029,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"e6b36175-447f-4272-94fc-cc0520d79014"},"source":["# Test your function; find positive words at or above a threshold (set label to 1)\n","get_words_by_threshold(freqs, label=1, threshold=10)"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{':)': {'negative': 2, 'positive': 2847, 'ratio': 949.3333333333334},\n"," ':-)': {'negative': 0, 'positive': 543, 'ratio': 544.0},\n"," ':D': {'negative': 0, 'positive': 498, 'ratio': 499.0},\n"," ':p': {'negative': 0, 'positive': 103, 'ratio': 104.0},\n"," ';)': {'negative': 0, 'positive': 22, 'ratio': 23.0},\n"," 'arriv': {'negative': 4, 'positive': 57, 'ratio': 11.6},\n"," 'bam': {'negative': 0, 'positive': 44, 'ratio': 45.0},\n"," 'blog': {'negative': 0, 'positive': 27, 'ratio': 28.0},\n"," 'commun': {'negative': 1, 'positive': 27, 'ratio': 14.0},\n"," 'fav': {'negative': 0, 'positive': 11, 'ratio': 12.0},\n"," 'fback': {'negative': 0, 'positive': 26, 'ratio': 27.0},\n"," 'flipkartfashionfriday': {'negative': 0, 'positive': 16, 'ratio': 17.0},\n"," 'followfriday': {'negative': 0, 'positive': 23, 'ratio': 24.0},\n"," 'glad': {'negative': 2, 'positive': 41, 'ratio': 14.0},\n"," \"here'\": {'negative': 0, 'positive': 20, 'ratio': 21.0},\n"," 'influenc': {'negative': 0, 'positive': 16, 'ratio': 17.0},\n"," 'pleasur': {'negative': 0, 'positive': 10, 'ratio': 11.0},\n"," 'shout': {'negative': 0, 'positive': 11, 'ratio': 12.0},\n"," 'stat': {'negative': 0, 'positive': 51, 'ratio': 52.0},\n"," 'via': {'negative': 1, 'positive': 60, 'ratio': 30.5},\n"," 'warsaw': {'negative': 0, 'positive': 44, 'ratio': 45.0},\n"," 'youth': {'negative': 0, 'positive': 14, 'ratio': 15.0}}"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"C9n3XvKtJgp8","colab_type":"text"},"source":["Emojis like :( and words like 'me' tend to have a negative connotation. Other words like 'glad', 'community', and 'arrives' tend to be found in the positive tweets."]},{"cell_type":"markdown","metadata":{"id":"qjW1IAqFJjux","colab_type":"text"},"source":["### **Error Analysis**"]},{"cell_type":"code","metadata":{"id":"v1D8AJc-JZW3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1593670281332,"user_tz":-480,"elapsed":1438,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"df02d898-9902-406d-ca10-394aeec59ff9"},"source":["print('Truth Predicted Tweet')\n","for x, y in zip(test_x, test_y):\n","    y_hat = predict_naive_bayes(x, logprior, loglikelihood)\n","    if y != (np.sign(y_hat) > 0):\n","        print('%d\\t%0.2f\\t%s' % (y, np.sign(y_hat) > 0, ' '.join(\n","            process_tweet(x)).encode('ascii', 'ignore')))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["Truth Predicted Tweet\n","1\t0.00\tb''\n","1\t0.00\tb'truli later move know queen bee upward bound movingonup'\n","1\t0.00\tb'new report talk burn calori cold work harder warm feel better weather :p'\n","1\t0.00\tb'harri niall 94 harri born ik stupid wanna chang :D'\n","1\t0.00\tb''\n","1\t0.00\tb''\n","1\t0.00\tb'park get sunlight'\n","1\t0.00\tb'uff itna miss karhi thi ap :p'\n","0\t1.00\tb'hello info possibl interest jonatha close join beti :( great'\n","0\t1.00\tb'u prob fun david'\n","0\t1.00\tb'pat jay'\n","0\t1.00\tb'whatev stil l young >:-('\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aF9h5kKkJwZg","colab_type":"text"},"source":["### **Play around with random tweet texts**"]},{"cell_type":"code","metadata":{"id":"IK-iRSV6JqJ6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593670348214,"user_tz":-480,"elapsed":958,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"cf1cfd8c-ba91-4de9-fcf9-3af2c5f1f104"},"source":["new_tweet = 'I am happy because I am learning NLP :)'\n","\n","p = predict_naive_bayes(new_tweet, logprior, loglikelihood)\n","print(p)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["9.574768961173337\n"],"name":"stdout"}]}]}
{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LR-LogisticRegression(SA).ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"12L1lbRBRoYoHGNNTlrHfuy5G6yBPfMKp","authorship_tag":"ABX9TyO+6oyHQ+NDOjxxxHi+Sn55"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"9CPyKs0PxqjQ","colab_type":"text"},"source":["## **Logistic Regression**"]},{"cell_type":"markdown","metadata":{"id":"XtngdPqbx4St","colab_type":"text"},"source":["In this notebook, wee will implement logistic regression for sentiment analysis on tweets. Given a tweet, we will decide if it has a positive sentiment or a negative sentiment. \n","\n","We will cover below concepts:\n","\n","* How to extract features for Logistic Regression given some text\n","* Implement Logistic Regression\n","* Apply Logistic Regression on a NLP task\n","* Test using our Logistic Regression\n","* Perform error analysis"]},{"cell_type":"markdown","metadata":{"id":"NqP5Vf4JyiYC","colab_type":"text"},"source":["### **Import/Load required libraries and data**"]},{"cell_type":"code","metadata":{"id":"oYf-R9dKy5hA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593451127308,"user_tz":-480,"elapsed":6614,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["! cp '/content/drive/My Drive/Colab Notebooks/utils.py' '/content'"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"2aW1-f4T1vqy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"executionInfo":{"status":"ok","timestamp":1593451152192,"user_tz":-480,"elapsed":4360,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"1d65d3f0-d1c1-4e4f-c789-4073ad6d6a55"},"source":["! cat 'utils.py' | tail -10"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","    for label, tweet in zip(labels_list, tweets):\n","        for word in process_tweet(tweet):\n","            pair = (word, label)\n","            freqs[pair] = freqs.get(pair, 0) + 1\n","\n","    return freqs"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1yGPJ3guxpuo","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593451159352,"user_tz":-480,"elapsed":1721,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["import nltk\n","from os import getcwd \n","import numpy as np\n","import pandas as pd\n","from nltk.corpus import twitter_samples\n","\n","from utils import process_tweet, build_freqs"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"18bibdu7zFoS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1593451163453,"user_tz":-480,"elapsed":2421,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"17655012-9ac7-4917-be4a-d006c6108b56"},"source":["nltk.download('twitter_samples')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/twitter_samples.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"rsK5iQhV0jPm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":70},"executionInfo":{"status":"ok","timestamp":1593451167770,"user_tz":-480,"elapsed":1475,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"88816a9c-92ec-41a2-a9d2-2e42f2c3494f"},"source":["nltk.download('stopwords')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"w4o_vfGIzNMr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593451171740,"user_tz":-480,"elapsed":1740,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["pos_tweets = twitter_samples.strings('positive_tweets.json')\n","neg_tweets = twitter_samples.strings('negative_tweets.json')"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"DQcw4F7QzXPy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593451174194,"user_tz":-480,"elapsed":1292,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["# split the dataset into train(80%) and test(20%)\n","train_pos = pos_tweets[:4000]\n","test_pos = pos_tweets[4000:]\n","train_neg = neg_tweets[:4000]\n","test_neg = neg_tweets[4000:]\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"XMpgnSChzs02","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593451176253,"user_tz":-480,"elapsed":1106,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["# create positive and negative labels\n","train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n","test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZofDmu60FIj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593451177680,"user_tz":-480,"elapsed":839,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"d8c5284c-1ba1-4a6c-bd34-25575a66daec"},"source":["# print the shape of train and test labels\n","print(\"train_y.shape =\" + str(train_y.shape))\n","print(\"test_y.shape =\" + str(test_y.shape))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["train_y.shape =(8000, 1)\n","test_y.shape =(2000, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mmyNXn5R0Wrr","colab_type":"text"},"source":["### **Create Frequency dictionary**"]},{"cell_type":"code","metadata":{"id":"O2-dgZSN0SGu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593451182416,"user_tz":-480,"elapsed":3744,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["freqs = build_freqs(train_x, train_y)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"goXybQ9S0emB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593451185670,"user_tz":-480,"elapsed":1141,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"8ebeb997-ba40-413c-a193-910415c31e59"},"source":["# check the output\n","print(\"type(freqs) =\" + str(type(freqs)))\n","print(\"len(freqs) =\" + str(len(freqs.keys())))"],"execution_count":12,"outputs":[{"output_type":"stream","text":["type(freqs) =<class 'dict'>\n","len(freqs) =11346\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kIe5Q-hRBJSM","colab_type":"text"},"source":["### **Process Tweets**"]},{"cell_type":"code","metadata":{"id":"bHoPYnVZBHBc","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":106},"executionInfo":{"status":"ok","timestamp":1593451191297,"user_tz":-480,"elapsed":1320,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"a06b0db0-863a-473c-950c-f4124868b218"},"source":["# test the process_tweet() function\n","print('This is an example of a positive class tweet: \\n', train_x[0])\n","print('\\nThis is an example of processed version of a positive class tweet: \\n', process_tweet(train_x[0]))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["This is an example of a positive class tweet: \n"," #FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n","\n","This is an example of processed version of a positive class tweet: \n"," ['followfriday', 'top', 'engag', 'member', 'commun', 'week', ':)']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"rkt7ozIPBmGf","colab_type":"text"},"source":["### **Logistic Regression**"]},{"cell_type":"markdown","metadata":{"id":"1-1Ib4laBrmQ","colab_type":"text"},"source":["#### **Sigmoid Function**"]},{"cell_type":"markdown","metadata":{"id":"fGcI-QswByDt","colab_type":"text"},"source":["The *Sigmoid* function is defined as:\n","\n","h(*z*) = 1 / (1 + exp(-*z*))"]},{"cell_type":"code","metadata":{"id":"R-2jXfXQBgLt","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593452480750,"user_tz":-480,"elapsed":1207,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def sigmoid(z): \n","    '''\n","    Returns the sigmoid of input value.\n","    Input:\n","        z: is the input (can be a scalar or an array)\n","    Output:\n","        h: the sigmoid of z\n","    '''\n","    h = 1 / (1 + np.exp(-z))\n","    return h"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"whiKFCaOCKOz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593451199194,"user_tz":-480,"elapsed":896,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"f17798c2-e522-42f2-84ee-b7456bf53ba9"},"source":["# test the sigmoid function\n","if (sigmoid(0) == 0.5):\n","    print('SUCCESS!')\n","else:\n","    print('Oops!')\n","\n","if (sigmoid(4.92) == 0.9927537604041685):\n","    print('CORRECT!')\n","else:\n","    print('Oops again!')"],"execution_count":15,"outputs":[{"output_type":"stream","text":["SUCCESS!\n","CORRECT!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Jc1r8d7YV1W9","colab_type":"text"},"source":["#### **Logistic Regression: Linear Regression and a Sigmoid**\n","Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n","\n","Regression:$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$ $\\theta$ values are simply \"weights\" that our model is supposed to learn.\n","\n","Logistic regression$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$We will refer to 'z' as the 'logits'.\n","\n","**Cost function and Gradient**\n","\n","The cost function used for logistic regression is the average of the log-loss across all training examples:\n","\n","$$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) $$\n","* $m$ is the number of training examples\n","* $y^{(i)}$ is the actual label of the i-th training example.\n","* $h(z(\\theta)^{(i)})$ is the model's prediction for the i-th training example.\n","\n","The loss function for a single training example is$$ Loss = -1 \\times \\left( y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)})) \\right)$$\n","\n","* All the $h$ values are between 0 and 1, so the logs will be negative. That's the reason for the factor of -1 applied to the sum of the two loss terms.\n","* Note that when the model predicts 1 ($h(z(\\theta)) = 1$) and the label $y$ is also 1, the loss for that training example is 0.\n","* Similarly, when the model predicts 0 ($h(z(\\theta)) = 0$) and the actual label is also 0, the loss for that training example is 0.\n","* However, when the model prediction is close to 1 ($h(z(\\theta)) = 0.9999$) and the label is 0, the second term of the log loss becomes a large negative number, which is then multiplied by the overall factor of -1 to convert it to a positive loss value. $-1 \\times (1 - 0) \\times log(1 - 0.9999) \\approx 9.2$ The closer the model prediction gets to 1, the larger the loss."]},{"cell_type":"code","metadata":{"id":"TD9wXSySCRUA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593451203392,"user_tz":-480,"elapsed":1209,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"4c0635e4-5f02-4270-b768-9f2bf4a91738"},"source":["# verify that when the model predicts close to 1, but the actual label is 0, the loss is a large positive value\n","-1 * (1 - 0) * np.log(1 - 0.9999) # loss is about 9.2"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.210340371976294"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"fLTrNj7lXXWt","colab_type":"text"},"source":["* Likewise, if the model predicts close to 0 ($h(z) = 0.0001$) but the actual label is 1, the first term in the loss function becomes a large number:\n","\n","  $-1 \\times log(0.0001) \\approx 9.2$. The closer the prediction is to zero, the larger the loss."]},{"cell_type":"code","metadata":{"id":"XtyEOabFXW9Z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593451208621,"user_tz":-480,"elapsed":3027,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"62a0dec9-d79a-4dc9-aa66-829ac155abf8"},"source":["# verify that when the model predicts close to 0 but the actual label is 1, the loss is a large positive value\n","-1 * np.log(0.0001) # loss is about 9.2"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9.210340371976182"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"EYqkcXr4X4JM","colab_type":"text"},"source":["**Update the weights**\n","\n","To update our weight vector $\\theta$, we will apply gradient descent to iteratively improve your model's predictions.\n","\n","The gradient of the cost function $J$ with respect to one of the weights $\\theta_j$ is:\n","\n","$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j $$\n","* 'i' is the index across all 'm' training examples.\n","* 'j' is the index of the weight $\\theta_j$, so $x_j$ is the feature associated with weight $\\theta_j$\n","\n","* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n","\n","* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be."]},{"cell_type":"markdown","metadata":{"id":"Nw5UFUyIYco_","colab_type":"text"},"source":["#### **Gradient Descent function**"]},{"cell_type":"markdown","metadata":{"id":"rOlrZbf_Ykme","colab_type":"text"},"source":["* The number of iterations *num_iters* is the number of times that we will use the entire training set.\n","* For each iteration, we will calculate the cost function using all training examples, and for all features.\n","* Instead of updating a single weight $\\theta_i$ at a time, we can update all the weights in the column vector:\n","$$\\mathbf{\\theta} = \\begin{pmatrix}\n","\\theta_0\n","\\\\\n","\\theta_1\n","\\\\ \n","\\theta_2 \n","\\\\ \n","\\vdots\n","\\\\ \n","\\theta_n\n","\\end{pmatrix}$$\n","* $\\mathbf{\\theta}$ has dimensions (n+1, 1), where 'n' is the number of features, and there is one more element for the bias term $\\theta_0$ (the corresponding feature value $\\mathbf{x_0}$ is 1).\n","* The 'logits', 'z', are calculated by multiplying the feature matrix 'x' with the weight vector 'theta'. $z = \\mathbf{x}\\mathbf{\\theta}$\n","  * $\\mathbf{x}$ has dimensions (m, n+1)\n","  * $\\mathbf{\\theta}$: has dimensions (n+1, 1)\n","  * $\\mathbf{z}$: has dimensions (m, 1)\n","* The prediction 'h', is calculated by applying the sigmoid to each element in 'z': $h(z) = sigmoid(z)$, and has dimensions (m,1).\n","* The cost function $J$ is calculated by taking the dot product of the vectors 'y' and 'log(h)'. Since both 'y' and 'h' are column vectors (m,1), transpose the vector to the left, so that matrix multiplication of a row vector with column vector performs the dot product.$$J = \\frac{-1}{m} \\times \\left(\\mathbf{y}^T \\cdot log(\\mathbf{h}) + \\mathbf{(1-y)}^T \\cdot log(\\mathbf{1-h}) \\right)$$\n","* The update of $\\theta$ is also vectorized. Because the dimensions of $\\mathbf{x}$ are (m, n+1), and both $\\mathbf{h}$ and $\\mathbf{y}$ are (m, 1), we need to transpose the $\\mathbf{x}$ and place it on the left in order to perform matrix multiplication, which then yields the (n+1, 1) answer we need:$$\\mathbf{\\theta} = \\mathbf{\\theta} - \\frac{\\alpha}{m} \\times \\left( \\mathbf{x}^T \\cdot \\left( \\mathbf{h-y} \\right) \\right)$$\n","* We will use np.dot for matrix multiplication.\n","* To ensure that the fraction -1/m is a decimal value, we will cast either the numerator or denominator (or both), like `float(1)`, or write `1.` for the float version of 1."]},{"cell_type":"code","metadata":{"id":"41KKGz0fX1sX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593451333851,"user_tz":-480,"elapsed":1271,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def gradientDescent(x, y, theta, alpha, num_iters):\n","    '''\n","    Returns the cost and weight vector.\n","    Input:\n","        x: matrix of features, i.e. (m,n+1)\n","        y: corresponding labels of the input matrix x, dimensions (m,1)\n","        theta: weight vector of dimension (n+1,1)\n","        alpha: learning rate\n","        num_iters: number of iterations we want to train your model for\n","    Output:\n","        J: the final cost\n","        theta: our final weight vector\n","    Hint: we might want to print the cost to make sure that it is going down.\n","    '''\n","    # get 'm', the number of rows in matrix x\n","    m = x.shape[0]\n","    \n","    for i in range(0, num_iters):\n","        \n","        # get z, the dot product of x and theta\n","        z = np.dot(x, theta)\n","        \n","        # get the sigmoid of z\n","        h = sigmoid(z)\n","        \n","        # calculate the cost function\n","        J = (-1/m) * (np.dot(y.T, np.log(h)) + np.dot((1 - y.T), np.log(1 - h)))\n","\n","        # update the weights theta\n","        theta = theta - (alpha/m) * np.dot(x.T, (h - y))\n","        \n","    J = float(J)\n","    return J, theta"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"xZV6DPGsXUfd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593440015019,"user_tz":-480,"elapsed":790,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"c3faab15-4416-4718-d3cb-489063c31944"},"source":["# test the function\n","np.random.seed(1)\n","# X input is 10 x 3 with ones for the bias terms\n","tmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n","# Y Labels are 10 x 1\n","tmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n","\n","# Apply gradient descent\n","tmp_J, tmp_theta = gradientDescent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\n","print(f\"The cost after training is {tmp_J:.8f}.\")\n","print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["The cost after training is 0.67094970.\n","The resulting vector of weights is [4.1e-07, 0.00035658, 7.309e-05]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"E2jGusEAIJAn","colab_type":"text"},"source":["### **Extracting Features**"]},{"cell_type":"markdown","metadata":{"id":"R4Xppy1xIN76","colab_type":"text"},"source":["* Given a list of tweets, we will extract the features and store them in a matrix. We will extract two feature:\n","  * First, the number of positive words in a tweet.\n","  * Second, the number of negative words in a tweet.\n","* Then we will train our Logistic Regression classifier on those features.\n","* Next we will test the classifier on a test dataset."]},{"cell_type":"markdown","metadata":{"id":"RNUc0NH7I1Uw","colab_type":"text"},"source":["#### ***extract_features()* function**"]},{"cell_type":"markdown","metadata":{"id":"G3zVLK96JO55","colab_type":"text"},"source":["* This function will take a single tweet at a time.\n","* It will process the tweet using *process_tweet()* function from *utils.py*, and save the list of tweet words.\n","* It will then loop through each word in the list of processed words, and do these:\n","  * For each word, check the *freqs* dictionary for the count when that word has a positive(1) label. *[Hint: key will be (word, 1.0)]*\n","  * Do the same thing when the word is associated with the negative(0) label. *[Hint: key will be (word, 0.0)]*\n","* <font color='orange'>We need to handle the case when (word, label) key is not found in the dictionary.\n","\n","  [*Hint: dictionary.get() function deals with case when key is not present*] </font>\n","  \n"]},{"cell_type":"code","metadata":{"id":"6n68ZT7vILhA","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593453410224,"user_tz":-480,"elapsed":1138,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def extract_features(tweet, freqs):\n","    '''\n","    Returns a feature vector of dimension (1,3), in the format [1.0, sum(positive_counts), sum(negative_counts)].\n","    Input: \n","        tweet: a list of words for one tweet\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    Output: \n","        x: a feature vector of dimension (1,3)\n","    '''\n","    # process_tweet tokenizes, stems, and removes stopwords\n","    word_l = process_tweet(tweet)\n","    \n","    # 3 elements in the form of a 1 x 3 vector\n","    x = np.zeros((1, 3)) \n","    \n","    #bias term is set to 1\n","    x[0,0] = 1 \n","    \n","    # loop through each word in the list of words\n","    for word in word_l:\n","        \n","        # increment the word count for the positive label 1\n","        x[0,1] += freqs.get((word, 1), 0)\n","        \n","        # increment the word count for the negative label 0\n","        x[0,2] += freqs.get((word, 0), 0)\n","        \n","    assert(x.shape == (1, 3))\n","    return x"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwPaOLo2LQQE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1593452889336,"user_tz":-480,"elapsed":1143,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"25e9fe2d-41c6-4e4d-e01b-a4f204e7d1da"},"source":["# test the function\n","\n","# test 1 (on training sample)\n","tmp1 = extract_features(train_x[0], freqs)\n","print('Test 1 result:\\n')\n","print(tmp1)\n","\n","# test 2 (when some words are not there in freqs dictionary)\n","tmp2 = extract_features('hooola foobar alpharomeo', freqs)\n","print('\\nTest 2 result:\\n')\n","print(tmp2)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Test 1 result:\n","\n","[[1.00e+00 3.02e+03 6.10e+01]]\n","\n","Test 2 result:\n","\n","[[1. 0. 0.]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"37e8OVHPMZ7M","colab_type":"text"},"source":["### **Train Logistic Regression classifier**"]},{"cell_type":"code","metadata":{"id":"g4yweXJgMYAi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1593453583611,"user_tz":-480,"elapsed":5181,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"3bfdb55c-5532-4ab0-8613-d0b2bc5991e3"},"source":["# collect the features 'x' and stack them into a matrix 'X'\n","X = np.zeros((len(train_x), 3))\n","for i in range(len(train_x)):\n","    X[i, :]= extract_features(train_x[i], freqs)\n","\n","# training labels corresponding to X\n","Y = train_y\n","\n","# Apply gradient descent\n","J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n","print(f\"The cost after training is {J:.8f}.\")\n","print(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta)]}\")"],"execution_count":35,"outputs":[{"output_type":"stream","text":["The cost after training is 0.24216529.\n","The resulting vector of weights is [7e-08, 0.0005239, -0.00055517]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"70JEr1ynPRI1","colab_type":"text"},"source":["### **Test the classifier**\n","\n","We will now test out logistic regression classifier on some input that the model has not seen before.\n","\n","#### **Create *predict_tweet()* function**\n","\n","This function will predict whether a tweet is positive or negative.\n","\n","**Steps:**\n","* Given a tweet, process it, and extract the features.\n","* Apply the learned model weights on the features to get the logits.\n","* Apply sigmoid to these logits to get prediction between 0 and 1."]},{"cell_type":"code","metadata":{"id":"OsUIabHdPAhV","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593453947236,"user_tz":-480,"elapsed":1093,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def predict_tweet(tweet, freqs, theta):\n","    '''\n","    Returns the predicted value between 0 and 1.\n","    Input: \n","        tweet: a string\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","        theta: (3,1) vector of weights\n","    Output: \n","        y_pred: the probability of a tweet being positive or negative\n","    '''\n","\n","    # extract the features of the tweet and store it into x\n","    x = extract_features(tweet, freqs)\n","    \n","    # make the prediction using x and theta\n","    y_pred = sigmoid(np.dot(x, theta))\n","    \n","    return y_pred"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"YlBS_hOSQaSS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":141},"executionInfo":{"status":"ok","timestamp":1593453967009,"user_tz":-480,"elapsed":1362,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"888aeee9-8724-4a46-b56e-a86de7db41cf"},"source":["# test the function\n","for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n","    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"],"execution_count":37,"outputs":[{"output_type":"stream","text":["I am happy -> 0.518580\n","I am bad -> 0.494339\n","this movie should have been great. -> 0.515331\n","great -> 0.515464\n","great great -> 0.530898\n","great great great -> 0.546273\n","great great great great -> 0.561561\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6_LRRBmyQuQC","colab_type":"text"},"source":["Now let's check the performance on our **test** dataset.\n","\n","To do that, we will create another function which will give us the accuracy of our classifier on test data.\n","\n","**Steps:**\n","* Given the test data and the weights of our trained model, we will calculate the accuracy of our logistic regression model.\n","* We will use our *predict_tweet()* function to make predictions on each tweet in the test set.\n","* If the prediction is > 0.5, we will set the model's classification y_hat to 1, otherwise the model's classification y_hat to 0.\n","* A prediction is accurate when y_hat equals test_y. We will sum up all the instances when they are equal and divide by m.\n","<font color='orange'>\n","* *Hints:*\n","  * np.asarray() to convert a list to a numpy array\n","  * np.squeeze() to make an (m,1) dimensional array into an (m,) array\n","</font>"]},{"cell_type":"code","metadata":{"id":"Wtypy0mBQfDp","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593454544095,"user_tz":-480,"elapsed":1003,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}}},"source":["def test_accuracy(test_x, test_y, freqs, theta):\n","    \"\"\"\n","    Input: \n","        test_x: a list of tweets\n","        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n","        freqs: a dictionary with the frequency of each pair (or tuple)\n","        theta: weight vector of dimension (3, 1)\n","    Output: \n","        accuracy: (number of tweets classified correctly) / (total number of tweets)\n","    \"\"\"\n","    \n","    # the list for storing predictions\n","    y_hat = []\n","    \n","    for tweet in test_x:\n","        # get the label prediction for the tweet\n","        y_pred = predict_tweet(tweet, freqs, theta)\n","        \n","        if y_pred > 0.5:\n","            # append 1.0 to the list\n","            y_hat.append(1.0)\n","        else:\n","            # append 0 to the list\n","            y_hat.append(0.0)\n","\n","    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n","    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n","    accuracy = np.sum(np.array(y_hat).reshape(-1, 1) == test_y) / len(test_y)\n","    \n","    return accuracy"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZiQ1eHcRSsBr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1593454585587,"user_tz":-480,"elapsed":1775,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"152f7c6b-62ac-40ca-9d01-c6b98bd64f88"},"source":["tmp_accuracy = test_accuracy(test_x, test_y, freqs, theta)\n","print(f\"Logistic regression model's test accuracy = {tmp_accuracy:.4f}\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Logistic regression model's test accuracy = 0.9950\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Wk-52kzPTMs1","colab_type":"text"},"source":["#### **Some playaround with random tweet texts**"]},{"cell_type":"code","metadata":{"id":"FnmkngnWS1-f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":212},"executionInfo":{"status":"ok","timestamp":1593454862502,"user_tz":-480,"elapsed":1133,"user":{"displayName":"Animesh S","photoUrl":"","userId":"13153667698297702858"}},"outputId":"7928f007-42ff-4b37-f26c-7ae4e66f16e7"},"source":["new_tweet = \"The movie was not good. The prequel was much better and awesome. This movie 0/10.\"\n","print('Tweet:')\n","print(new_tweet)\n","print('\\nProcessed Tweet:')\n","print(process_tweet(new_tweet), '\\n')\n","y_hat = predict_tweet(new_tweet, freqs, theta)\n","print('\\nPrediction Score:')\n","print(y_hat, '\\n')\n","if y_hat > 0.5:\n","    print('Positive sentiment')\n","else: \n","    print('Negative sentiment')"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Tweet:\n","The movie was not good. The prequel was much better and awesome. This movie 0/10.\n","\n","Processed Tweet:\n","['movi', 'good', 'prequel', 'much', 'better', 'awesom', 'movi', '0/10'] \n","\n","\n","Prediction Score:\n","[[0.51151624]] \n","\n","Positive sentiment\n"],"name":"stdout"}]}]}